{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bf7904-fc38-4e15-94f5-98c0d8be167a",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA) and Introduction to Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdfbabb-0954-47e9-bc92-e9ed1da2ff13",
   "metadata": {},
   "source": [
    "# What is EDA?\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the process of investigating datasets to discover patterns, spot anomalies,\n",
    "test hypotheses, and check assumptions using statistical summaries and visualizations.\n",
    "\n",
    "\n",
    "## Why is EDA Important?\n",
    "\n",
    "- Understand the structure and meaning of data\n",
    "- Detect errors, missing values, and outliers\n",
    "- Highlight important relationships between variables\n",
    "- Prepare the dataset for machine learning (ML) modeling\n",
    "\n",
    "\n",
    "#  Data Systems: Local vs Distributed\n",
    "\n",
    " 1. Local System (Database on Laptop)\n",
    "\n",
    "- Runs on a single machine\n",
    "- Limited storage and compute\n",
    "- Suitable for small or medium data\n",
    "- Example: SQLite, MySQL running on your laptop\n",
    "\n",
    "\n",
    " 2. Distributed System (Hadoop, Spark)\n",
    "\n",
    "- Runs on multiple machines\n",
    "- Can store and process big data\n",
    "- Examples: Hadoop Distributed File System (HDFS), Apache Spark, Hive\n",
    "\n",
    "\n",
    "#  Real-World Example: Local Shops & Central Database\n",
    "\n",
    "**Imagine:**\n",
    "\n",
    "- 3 local grocery stores with individual databases\n",
    "- Each store performs ETL (Extract, Transform, Load)\n",
    "- All data is sent to a central master database or data warehouse\n",
    "- After cleaning and transformation, a final combined dataset is prepared\n",
    "\n",
    "\n",
    "#  EDA Techniques – A Deep Dive into Feature Engineering\n",
    "\n",
    "**List of 7 Core EDA / Feature Engineering Techniques**\n",
    "\n",
    "1. Variable Identification\n",
    "2. Univariate Analysis\n",
    "3. Bivariate Analysis\n",
    "4. Outlier Detection\n",
    "5. Missing Value Treatment\n",
    "6. Variable Transformation\n",
    "7. Variable Creation\n",
    "\n",
    "\n",
    "## Variable Identification\n",
    "\n",
    "**Classify features into:**\n",
    "\n",
    "  - Independent Variables (X): used to predict\n",
    "  - Dependent Variable (Y): the target/output\n",
    "\n",
    "  **Types of Variables:**\n",
    "\n",
    "  - Categorical: Gender, City\n",
    "  - Numerical: Age, Salary\n",
    "  - Date/Time: Timestamp, DOB\n",
    "\n",
    "\n",
    "**Family Example:**\n",
    "\n",
    "Family has 4 members:\n",
    "\n",
    "- Dad (earns money)\n",
    "  \n",
    "- Mom (housewife)\n",
    "\n",
    "- Son (student)\n",
    "\n",
    "- Daughter (student)\n",
    "\n",
    "Only Dad earns → he is the dependent variable (Y)\n",
    "\n",
    "Others are independent variables: Mom (X1), Son (X2), Daughter (X3)\n",
    "\n",
    "So, in ML form:\n",
    "\n",
    "Y = X1 + X2 + X3  (like Multiple Linear Regression)\n",
    "\n",
    "##  Univariate Analysis\n",
    "\n",
    "Analyzing one variable at a time.\n",
    "\n",
    "- Categorical: use bar charts, value_counts()\n",
    "- Numerical: use histograms, boxplots, describe()\n",
    "\n",
    "\n",
    "## Bivariate Analysis\n",
    "\n",
    "Studying the relationship between two variables.\n",
    "\n",
    "- Categorical vs Categorical: Stacked bar plot, crosstab\n",
    "- Numerical vs Numerical: Scatter plot, correlation\n",
    "- Categorical vs Numerical: Boxplot\n",
    "\n",
    "**Correlation:**\n",
    "Correlation is a statistical measure that expresses the extent to which two variables are linearly related.\n",
    "\n",
    "It ranges between -1 and +1.\n",
    "\n",
    "- **+1** → Perfect positive correlation\n",
    "- **-1** → Perfect negative correlation\n",
    "- **0** → No correlation (zero correlation)\n",
    "\n",
    "**Types of Correlation:**\n",
    "\n",
    "- Positive Correlation: As one variable increases, the other also increases (e.g., height vs weight)\n",
    "  \n",
    "- Negative Correlation: As one variable increases, the other decreases (e.g., exercise time vs weight)\n",
    "\n",
    "- Zero Correlation: No linear relationship between the two variables\n",
    "\n",
    "##  Outlier Detection\n",
    "\n",
    "Outliers are values far from the rest. \n",
    "\n",
    "Outliers will inpact classification  algorithms , LR and KNN\n",
    "\n",
    "**Methods**\n",
    "\n",
    "- Boxplot (outside whiskers)\n",
    "- Z-score\n",
    "- IQR method\n",
    "\n",
    "Example using IQR:\n",
    "\n",
    "Q1 = df['Age'].quantile(0.25)\n",
    "\n",
    "Q3 = df['Age'].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = df[(df['Age'] < Q1 - 1.5*IQR) | (df['Age'] > Q3 + 1.5*IQR)]\n",
    "\n",
    "\n",
    "##  Missing Value Treatment\n",
    "\n",
    "Ways to handle nulls:\n",
    "\n",
    "- Delete rows/columns (Numerical Data & Catogorical Data)\n",
    "  \n",
    "- Impute (mean, median, mode) (Numerical Data)\n",
    "\n",
    "- Forward/Backward fill (Numerical Data)\n",
    "\n",
    "- Mode and KNN Impute (Catogorical Data)\n",
    "\n",
    "\n",
    "Example:\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "##  Variable Transformation\n",
    "\n",
    "Changing the scale or format of variables to make them suitable for modeling.\n",
    "\n",
    "### Scaling:\n",
    "\n",
    "- MinMaxScaler: Scales data between 0 and 1\n",
    "- StandardScaler: Standardizes data to have mean = 0 and std = 1\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df['Age_scaled'] = scaler.fit_transform(df[['Age']])\n",
    "```\n",
    "\n",
    "### Encoding Categorical Variables:\n",
    "\n",
    "- Label Encoding: Assigns numeric labels to categories (e.g., Male = 0, Female = 1)\n",
    "- One-Hot Encoding: Creates binary (0/1) columns for each category\n",
    "- Dummy Variables: A form of one-hot encoding used to avoid dummy variable trap (remove one column)\n",
    "\n",
    "\n",
    "##  Variable Creation\n",
    "\n",
    "Creating new features from existing ones:\n",
    "\n",
    "- Combine features\n",
    "  \n",
    "- Extract info from date\n",
    "  \n",
    "- BMI = weight / height^2\n",
    "\n",
    "Example:\n",
    "df['BMI'] = df['Weight_kg'] / (df['Height_m'] ** 2)\n",
    "\n",
    "##  Summary Table\n",
    "\n",
    "| Step | Technique Name          | Purpose                                      |\n",
    "|------|--------------------------|----------------------------------------------|\n",
    "| 1    | Variable Identification  | Define roles of features (X vs Y)            |\n",
    "| 2    | Univariate Analysis      | Understand individual variable distributions |\n",
    "| 3    | Bivariate Analysis       | Analyze relationships between variables      |\n",
    "| 4    | Outlier Detection        | Detect extreme values                        |\n",
    "| 5    | Missing Value Treatment  | Handle null values                           |\n",
    "| 6    | Variable Transformation  | Rescale/encode features                      |\n",
    "| 7    | Variable Creation        | Create informative new features              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bdcc7-bb0c-4a29-b026-4255d9b01569",
   "metadata": {},
   "source": [
    "**Introduction to Machine Learning (ML)**\n",
    "\n",
    " **Why Learn ML After EDA?**\n",
    "\n",
    "- EDA helps us understand and prepare data.\n",
    "- Without clean and understood data, ML models won’t work well.\n",
    "- EDA decides how data is transformed, which features are used, and helps choose the right ML model.\n",
    "\n",
    "\n",
    "# ML Categories\n",
    "\n",
    "## Regression\n",
    "- **Used when the dependent variable is continuous** (e.g., price, temperature)\n",
    "- Examples: Gold price, petrol price, house price, stock price, weather, crypto, etc.\n",
    "\n",
    "We use **regression models** in such cases.\n",
    "\n",
    "### Regression Algorithms:\n",
    "1. Simple Linear Regression\n",
    "2. Multiple Linear Regression\n",
    "3. Polynomial Regression\n",
    "4. Gradient Descent\n",
    "5. Stochastic Gradient Descent\n",
    "6. Batch Gradient Descent\n",
    "7. Lasso Regularization (L1)\n",
    "8. Ridge Regularization (L2)\n",
    "9. Elastic Net (L1 + L2)\n",
    "10. K-Nearest Neighbor Regression (KNN)\n",
    "11. Decision Tree Regression\n",
    "12. Random Forest Regression\n",
    "13. ANN Regression\n",
    "14. Time Series Analysis\n",
    "15. XGBoost Regression\n",
    "16. LGBM Regressor\n",
    "17. Support Vector Regressor (SVR)\n",
    "\n",
    "\n",
    "\n",
    "##  Classification\n",
    "- **Used when the dependent variable is categorical or binary**\n",
    "- Examples: Win/Loss, Pass/Fail, Spam/Not Spam, Rain/No Rain, Yes/No\n",
    "\n",
    "We use **classification models** in such cases.\n",
    "\n",
    "### Classification Algorithms (Only Names):\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN)\n",
    "3. Decision Tree Classifier\n",
    "4. Random Forest Classifier\n",
    "5. Naive Bayes\n",
    "6. Support Vector Machine (SVM)\n",
    "7. Stochastic Gradient Descent Classifier\n",
    "8. Gradient Boosting Classifier\n",
    "9. XGBoost Classifier\n",
    "10. LGBM Classifier\n",
    "11. AdaBoost\n",
    "12. Extra Trees Classifier\n",
    "13. ANN Classifier\n",
    "14. CNN (for images)\n",
    "15. RNN (for sequences)\n",
    "16. CatBoost\n",
    "17. Voting Classifier\n",
    "\n",
    "\n",
    "\n",
    "## Clustering\n",
    "- **No dependent variable** (unsupervised learning)\n",
    "- Use case: Grouping customers, documents, patterns, etc.\n",
    "\n",
    "### Clustering Algorithms (Only Names):\n",
    "1. K-Means\n",
    "2. DBSCAN\n",
    "3. Agglomerative Clustering\n",
    "4. Hierarchical Clustering\n",
    "5. Mean Shift\n",
    "6. OPTICS\n",
    "7. Gaussian Mixture Model (GMM)\n",
    "\n",
    "\n",
    "**Important Tip:**\n",
    "\n",
    ">  **Choosing the right dependent variable (Y) is CRUCIAL**\n",
    "\n",
    "- It decides whether your problem is a regression or classification task\n",
    "- Example:\n",
    "  - Data: `x1 = name`, `x2 = soft`, `x3 = new/old`, `x4 = hospital`, `x5 = purchased`, `y = price`\n",
    "  - If **y = price** → Regression\n",
    "  - If **y = purchased (yes/no)** → Classification\n",
    "\n",
    "**Attribute Relevance**\n",
    "\n",
    "- Two types:\n",
    "  \n",
    "  - **Relevant Attributes**: Improve model quality\n",
    "  - **Irrelevant Attributes**: Cause noise, overfitting, multicollinearity\n",
    "\n",
    "To build a strong ML model:\n",
    "- Use only relevant variables\n",
    "- Remove noise to reduce overfitting & errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544c972-b724-4b97-86fb-30d0b15b58bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
